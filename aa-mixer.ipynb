{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8d5003ba",
   "metadata": {},
   "source": [
    "---\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3a673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp aa_mixer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4d264b",
   "metadata": {},
   "source": [
    "# aa_mixer\n",
    "\n",
    "> Trying to map audio embeddings to vector spaces, for mixing.\n",
    "\n",
    "We try to make the sum of the embeddings of solo parts, equal(/close) to the embedding of the sum (i.e. the full mix).\n",
    "\n",
    "Based on `accelerate`-powered code by Zach Evans & Katherine Crowson, cf. https://github.com/zqevans/audio-diffusion/blob/main/train_diffgan_accel.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ce10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-78486s4v because the default path (/home/shawley/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from prefigure.prefigure import get_all_args, push_wandb_config\n",
    "from copy import deepcopy\n",
    "import math\n",
    "import json\n",
    "import subprocess\n",
    "import os, sys\n",
    "\n",
    "import accelerate\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import optim, nn, Tensor\n",
    "from torch import multiprocessing as mp\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data as torchdata\n",
    "from tqdm import tqdm, trange\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "import wandb\n",
    "\n",
    "from aeiou.viz import embeddings_table, pca_point_cloud, audio_spectrogram_image, tokens_spectrogram_image\n",
    "from aeiou.hpc import load, save, HostPrinter, freeze\n",
    "from aeiou.datasets import AudioDataset\n",
    "\n",
    "# audio-diffusion imports\n",
    "import pytorch_lightning as pl\n",
    "from diffusion.pqmf import CachedPQMF as PQMF\n",
    "from encoders.encoders import AttnResEncoder1D\n",
    "from autoencoders.soundstream import SoundStreamXLEncoder\n",
    "from dvae.residual_memcodes import ResidualMemcodes\n",
    "from decoders.diffusion_decoder import DiffusionAttnUnet1D\n",
    "from diffusion.model import ema_update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda07ff",
   "metadata": {},
   "source": [
    "## Set up AutoEncoder Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad247898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#audio-diffusion stuff \n",
    "# Define the noise schedule and sampling loop\n",
    "def get_alphas_sigmas(t):\n",
    "    \"\"\"Returns the scaling factors for the clean image (alpha) and for the\n",
    "    noise (sigma), given a timestep.\"\"\"\n",
    "    return torch.cos(t * math.pi / 2), torch.sin(t * math.pi / 2)\n",
    "\n",
    "def get_crash_schedule(t):\n",
    "    sigma = torch.sin(t * math.pi / 2) ** 2\n",
    "    alpha = (1 - sigma ** 2) ** 0.5\n",
    "    return alpha_sigma_to_t(alpha, sigma)\n",
    "\n",
    "def alpha_sigma_to_t(alpha, sigma):\n",
    "    \"\"\"Returns a timestep, given the scaling factors for the clean image and for\n",
    "    the noise.\"\"\"\n",
    "    return torch.atan2(sigma, alpha) / math.pi * 2\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample(model, x, steps, eta, logits):\n",
    "    \"\"\"Draws samples from a model given starting noise.\"\"\"\n",
    "    ts = x.new_ones([x.shape[0]])\n",
    "\n",
    "    # Create the noise schedule\n",
    "    t = torch.linspace(1, 0, steps + 1)[:-1]\n",
    "\n",
    "    t = get_crash_schedule(t)\n",
    "    \n",
    "    alphas, sigmas = get_alphas_sigmas(t)\n",
    "\n",
    "    # The sampling loop\n",
    "    for i in trange(steps):\n",
    "\n",
    "        # Get the model output (v, the predicted velocity)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            v = model(x, ts * t[i], logits).float()\n",
    "\n",
    "        # Predict the noise and the denoised image\n",
    "        pred = x * alphas[i] - v * sigmas[i]\n",
    "        eps = x * sigmas[i] + v * alphas[i]\n",
    "\n",
    "        # If we are not on the last timestep, compute the noisy image for the\n",
    "        # next timestep.\n",
    "        if i < steps - 1:\n",
    "            # If eta > 0, adjust the scaling factor for the predicted noise\n",
    "            # downward according to the amount of additional noise to add\n",
    "            ddim_sigma = eta * (sigmas[i + 1]**2 / sigmas[i]**2).sqrt() * \\\n",
    "                (1 - alphas[i]**2 / alphas[i + 1]**2).sqrt()\n",
    "            adjusted_sigma = (sigmas[i + 1]**2 - ddim_sigma**2).sqrt()\n",
    "\n",
    "            # Recombine the predicted noise and predicted denoised image in the\n",
    "            # correct proportions for the next step\n",
    "            x = pred * alphas[i + 1] + eps * adjusted_sigma\n",
    "\n",
    "            # Add the correct amount of fresh noise\n",
    "            if eta:\n",
    "                x += torch.randn_like(x) * ddim_sigma\n",
    "\n",
    "    # If we are on the last timestep, output the denoised image\n",
    "    return pred\n",
    "\n",
    "\n",
    "\n",
    "class DiffusionDVAE(pl.LightningModule):\n",
    "    def __init__(self, global_args):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pqmf_bands = global_args.pqmf_bands\n",
    "\n",
    "        if self.pqmf_bands > 1:\n",
    "            self.pqmf = PQMF(2, 70, global_args.pqmf_bands)\n",
    "\n",
    "        capacity = 32\n",
    "\n",
    "        c_mults = [2, 4, 8, 16, 32]\n",
    "        \n",
    "        strides = [4, 4, 2, 2, 2]\n",
    "\n",
    "        self.encoder = SoundStreamXLEncoder(\n",
    "            in_channels=2*global_args.pqmf_bands, \n",
    "            capacity=capacity, \n",
    "            latent_dim=global_args.latent_dim,\n",
    "            c_mults = c_mults,\n",
    "            strides = strides\n",
    "        )\n",
    "        self.encoder_ema = deepcopy(self.encoder)\n",
    "\n",
    "        self.diffusion = DiffusionAttnUnet1D(\n",
    "            io_channels=2, \n",
    "            cond_dim = global_args.latent_dim, \n",
    "            pqmf_bands = global_args.pqmf_bands, \n",
    "            n_attn_layers=4, \n",
    "            c_mults=[256, 256]+[512]*12\n",
    "        )\n",
    "\n",
    "        self.diffusion_ema = deepcopy(self.diffusion)\n",
    "        self.rng = torch.quasirandom.SobolEngine(1, scramble=True)\n",
    "        self.ema_decay = global_args.ema_decay\n",
    "        \n",
    "        self.num_quantizers = global_args.num_quantizers\n",
    "        if self.num_quantizers > 0:\n",
    "            quantizer_class = ResidualMemcodes if global_args.num_quantizers > 1 else Memcodes\n",
    "            \n",
    "            quantizer_kwargs = {}\n",
    "            if global_args.num_quantizers > 1:\n",
    "                quantizer_kwargs[\"num_quantizers\"] = global_args.num_quantizers\n",
    "\n",
    "            self.quantizer = quantizer_class(\n",
    "                dim=global_args.latent_dim,\n",
    "                heads=global_args.num_heads,\n",
    "                num_codes=global_args.codebook_size,\n",
    "                temperature=1.,\n",
    "                **quantizer_kwargs\n",
    "            )\n",
    "\n",
    "            self.quantizer_ema = deepcopy(self.quantizer)\n",
    "            \n",
    "        self.demo_reals_shape = None #overwrite thie later\n",
    "\n",
    "    def encode(self, *args, **kwargs):\n",
    "        if self.training:\n",
    "            return self.encoder(*args, **kwargs)\n",
    "        return self.encoder_ema(*args, **kwargs)\n",
    "\n",
    "    def decode(self, *args, **kwargs):\n",
    "        if self.training:\n",
    "            return self.diffusion(*args, **kwargs)\n",
    "        return self.diffusion_ema(*args, **kwargs)\n",
    "    \n",
    "    def encode_it(self, demo_reals):\n",
    "        encoder_input = demo_reals\n",
    "\n",
    "        if self.pqmf_bands > 1:\n",
    "            encoder_input = self.pqmf(demo_reals)\n",
    "\n",
    "        encoder_input = encoder_input.to(self.device)\n",
    "        self.demo_reals_shape = demo_reals.shape\n",
    "        \n",
    "        # noise is only used for decoding tbh!\n",
    "        #noise = torch.randn([demo_reals.shape[0], 2, self.demo_samples]).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.encoder_ema(encoder_input)\n",
    "            if self.quantized:\n",
    "                embeddings = rearrange(embeddings, 'b d n -> b n d') # Rearrange for Memcodes\n",
    "                embeddings, _= self.quantizer_ema(embeddings)\n",
    "                embeddings = rearrange(embeddings, 'b n d -> b d n')\n",
    "\n",
    "            embeddings = torch.tanh(embeddings)\n",
    "            return embeddings#, noise\n",
    "        \n",
    "    def decode_it(self, embeddings, demo_batch_size=None, demo_steps=35):\n",
    "        if None==demo_batch_size: demo_batch_size = self.demo_reals_shape[0]\n",
    "        noise = torch.randn([self.demo_reals_shape[0], 2, self.demo_samples]).to(self.device)\n",
    "        fake_batches = sample(self.diffusion_ema, noise, demo_steps, 0, embeddings)\n",
    "        audio_out = rearrange(fake_batches, 'b d n -> d (b n)') # Put the demos together\n",
    "        return audio_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc05fe18",
   "metadata": {},
   "source": [
    "Download the checkpoint file for the dvae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff6f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "on_colab = os.path.exists('/content')\n",
    "if on_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/') \n",
    "    ckpt_file = '/content/drive/MyDrive/AI/checkpoints/epoch=53-step=200000.ckpt'\n",
    "else:\n",
    "    ckpt_file = 'checkpoint.ckpt'\n",
    "    if not os.path.exists(ckpt_file):\n",
    "        url = 'https://drive.google.com/file/d/1C3NMdQlmOcArGt1KL7pH32KtXVCOfXKr/view?usp=sharing'\n",
    "        # downloading large files from GDrive requires special treatment to bypass the dialog button it wants to throw up\n",
    "        id = url.split('/')[-2]\n",
    "        cmd = f'wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \\'https://docs.google.com/uc?export=download&id={id}\\' -O- | sed -rn \\'s/.*confirm=([0-9A-Za-z_]+).*/\\1\\\\n/p\\')&id={id}\" -O {ckpt_file} && rm -rf /tmp/cookies.txt'\n",
    "        print(\"cmd = \\n\",cmd)\n",
    "        subprocess.run(cmd, shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed450521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n",
      "Autoencoder is ready to go!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(\"device = \",device)\n",
    "if torch.backends.mps.is_available():\n",
    "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "    \n",
    "args_dict = {'num_quantizers':0, 'sample_size': 65536, 'sample_rate':48000, 'latent_dim': 64, 'pqmf_bands':1, 'ema_decay':0.995, 'num_quantizers':0}\n",
    "#global_args = namedtuple(\"global_args\", args_dict.keys())(*args_dict.values())\n",
    "class DictObj:\n",
    "    def __init__(self, in_dict:dict):\n",
    "        assert isinstance(in_dict, dict), \"in_dict is not a dict\"\n",
    "        for key, val in in_dict.items():\n",
    "            if isinstance(val, (list, tuple)):\n",
    "               setattr(self, key, [DictObj(x) if isinstance(x, dict) else x for x in val])\n",
    "            else:\n",
    "               setattr(self, key, DictObj(val) if isinstance(val, dict) else val)\n",
    "\n",
    "global_args = DictObj(args_dict)\n",
    "\n",
    "\n",
    "autoencoder = DiffusionDVAE.load_from_checkpoint(ckpt_file, global_args=global_args)\n",
    "autoencoder.eval() # disable randomness, dropout, etc...\n",
    "\n",
    "\n",
    "# attach some arg values to the model \n",
    "autoencoder.demo_samples = global_args.sample_size \n",
    "autoencoder.quantized = global_args.num_quantizers > 0\n",
    "#model.device = device \n",
    "\n",
    "autoencoder = autoencoder.to(device)\n",
    "freeze(autoencoder)  # freeze the weights for inference\n",
    "print(\"Autoencoder is ready to go!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74d7d92",
   "metadata": {},
   "source": [
    "## The AA-mixer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a6cec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf https://github.com/tyunist/memory_efficient_mish_swish\n",
    "class Mish_func(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i * torch.tanh(F.softplus(i))\n",
    "        ctx.save_for_backward(i)\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        i = ctx.saved_tensors[0] \n",
    "        v = 1. + i.exp()\n",
    "        h = v.log() \n",
    "        grad_gh = 1./h.cosh().pow_(2) \n",
    "        grad_hx = i.sigmoid()\n",
    "        grad_gx = grad_gh *  grad_hx #grad_hv * grad_vx \n",
    "        grad_f =  torch.tanh(F.softplus(i)) + i * grad_gx    \n",
    "        return grad_output * grad_f \n",
    "\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    def forward(self, input_tensor):\n",
    "        return Mish_func.apply(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99f8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class EmbedBlock(nn.Module):\n",
    "    def __init__(self, dims:int, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(dims, dims, **kwargs)\n",
    "        #self.act = nn.LeakyReLU()\n",
    "        self.act = F.relu # Mish()\n",
    "        self.bn = nn.BatchNorm1d(dims)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.lin(x)\n",
    "        #x = rearrange(x, 'b d n -> b n d') # gotta rearrange for bn\n",
    "        #x = self.bn(x)\n",
    "        #x = rearrange(x, 'b n d -> b d n') # and undo rearrange for later layers\n",
    "        return self.act(x)\n",
    "\n",
    "\n",
    "class AudioAlgebra(nn.Module):\n",
    "    \"\"\"\n",
    "    Main AudioAlgebra model\n",
    "    \"\"\"\n",
    "    def __init__(self, global_args, device, enc_model, trivial=True):\n",
    "        super().__init__()\n",
    "        self.trivial = trivial      # trivial=True means trivial (i.e. no) re-embedding\n",
    "        self.device = device\n",
    "        self.enc_model = enc_model\n",
    "        self.dims = global_args.latent_dim\n",
    "        self.sample_size = global_args.sample_size\n",
    "        self.num_quantizers = global_args.num_quantizers\n",
    "\n",
    "        self.reembedding = nn.Sequential(  # something simple at first\n",
    "            #EmbedBlock(self.dims),\n",
    "            #EmbedBlock(self.dims),\n",
    "            #EmbedBlock(self.dims),\n",
    "            #EmbedBlock(self.dims),\n",
    "            #EmbedBlock(self.dims),\n",
    "            nn.Linear(self.dims,self.dims)\n",
    "            )\n",
    "\n",
    "    def forward(self,\n",
    "        stems:list,   # list of torch tensors denoting (chunked) solo audio parts to be mixed together\n",
    "        faders:list   # list of gain values to be applied to each stem\n",
    "        ):\n",
    "        \"\"\"We're going to 'on the fly' mix the stems according to the fader settings and generate\n",
    "        frozen-encoder embeddings for each (fader-adjusted) stem and for the total mix.\n",
    "        \"z0\" denotes an embedding from the frozen encoder, \"z\" denotes re-mapped embeddings\n",
    "        in (hopefully) the learned vector space\"\"\"\n",
    "        with torch.cuda.amp.autocast():\n",
    "            zs, z0s, zsum, z0sum = [], [], None, None\n",
    "            mix = torch.zeros_like(stems[0]).float()\n",
    "            for s, f in zip(stems, faders):   # encode a bunch of stems at different fader settings\n",
    "                mix_s = s * f                 # audio stem adjusted by gain fader f\n",
    "                with torch.no_grad():\n",
    "                    z0 = self.enc_model.encode_it(mix_s)  # encode the stem\n",
    "                z0sum = z0 if z0sum is None else z0sum + z0 \n",
    "                z0 = rearrange(z0, 'b d n -> b n d')\n",
    "                #----------------------------------\n",
    "                z = z0 if self.trivial else self.reembedding(z0).float()   # <-- this is the main work of the model\n",
    "                #----------------------------------\n",
    "                zsum = z if zsum is None else zsum + z # compute the sum of all the z's. we'll end up using this in our (metric) loss as \"pred\"\n",
    "                mix += mix_s              # save a record of full audio mix\n",
    "                zs.append(z)              # save a list of individual z's\n",
    "                z0s.append(z0)            # save a list of individual z0's\n",
    "\n",
    "            with torch.no_grad():\n",
    "                z0mix = self.enc_model.encode_it(mix)  # encode the mix\n",
    "            z0mix = rearrange(z0mix, 'b d n -> b n d')\n",
    "            zmix = self.reembedding(z0mix).float()        # map that according to our learned re-embedding. this will be the \"target\" in the metric loss\n",
    "            z0mix = rearrange(z0mix, 'b n d -> b d n')\n",
    "            \n",
    "            archive = {'zs':zs, 'mix':mix, 'znegsum':None, 'z0s': z0s, 'z0sum':z0sum, 'z0mix':z0mix}\n",
    "\n",
    "        return zsum, zmix, archive    # zsum = pred, zmix = target, and \"archive\" of extra stuff zs & zmix are just for extra info\n",
    "\n",
    "\n",
    "    def mag(self, v):\n",
    "        return torch.norm( v, dim=(1,2) ) # L2 / Frobenius / Euclidean\n",
    "\n",
    "    def distance(self, pred, targ):\n",
    "        return self.mag(pred - targ)\n",
    "    \n",
    "\n",
    "    def loss(self, zsum, zmix, archive, margin=1.0, loss_type='noshrink'):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            dist = self.distance(zsum, zmix) # for each member of batch, compute distance\n",
    "            loss = (dist**2).mean()  # mean across batch; so loss range doesn't change w/ batch_size hyperparam\n",
    "            if ('triplet'==loss_type) and (archive['znegsum'] is not None):\n",
    "                negdist = self.distance(archive['znegsum'], zmix)\n",
    "                negdist = negdist * (negdist < margin)   # beyond margin, do nothing\n",
    "                loss = F.relu( (dist**2).mean() - (negdist**2).mean() ) # relu gets us hinge of L2\n",
    "            if ('noshrink' == loss_type):                       # TODO: THIS DOESN\"T HELP try to preserve original magnitudes of of vectors \n",
    "                magdiffs2 = [ ( self.mag(z) - self.mag(z0) )**2 for (z,z0) in zip(archive['zs'], archive['z0s']) ]\n",
    "                loss += 1/300*(sum(magdiffs2)/len(magdiffs2)).mean() # mean of l2 of diff in vector mag  extra .mean() for good measure  \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b64f2",
   "metadata": {},
   "source": [
    "### Reconstruction /demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b84a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export \n",
    "def aa_demo(autoencoder, log_dict, zsum, zmix, step, demo_steps=35, sr=48000):\n",
    "    \"log decoded audio for zsum and zmix\"\n",
    "    for var,name in zip([zsum, zmix],['zsum','zmix']):\n",
    "        fake_audio = autoencoder.decode_it(var, demo_steps=demo_steps)\n",
    "        filename = f'{name}_{step:08}.wav'\n",
    "        fake_audio = fake_audio.clamp(-1, 1).mul(32767).to(torch.int16).cpu()\n",
    "        torchaudio.save(filename, fake_audio, self.sample_rate)\n",
    "        log_dict[name] = wandb.Audio(filename, sample_rate=sr, caption=name)   \n",
    "        #log_dict[f'{name}_spec'] = wandb.Image( tokens_spectrogram_image(var.detach()) )\n",
    "    return log_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8eedc9",
   "metadata": {},
   "source": [
    "### get_stems_faders:\n",
    "really this is more of a `dataloader` utility but for now its being called from the main loop because it involves less change to the dataloader. ;-) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def get_stems_faders(batch, dl:torchdata.DataLoader, maxstems=6):\n",
    "    \"grab some more audio stems and set fader values\"\n",
    "    nstems = 1 + int(torch.randint(maxstems-1,(1,1))[0][0].numpy()) # an int between 1 and maxstems, PyTorch style :-/\n",
    "    #print(\"nstems =\",nstems)\n",
    "    faders = 2*torch.rand(nstems)-1  # fader gains can be from -1 to 1\n",
    "    stems = [batch]\n",
    "    dl_iter = iter(dl)\n",
    "    for i in range(nstems-1):\n",
    "        stems.append(next(dl_iter)) \n",
    "    return stems, faders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16a1028",
   "metadata": {},
   "source": [
    "# Main run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a08186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96mSetting up AA model using device: cuda\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "accelerator = accelerate.Accelerator()\n",
    "device = accelerator.device\n",
    "hprint = HostPrinter(accelerator)\n",
    "hprint(f\"Setting up AA model using device: {device}\")\n",
    "aa_model = AudioAlgebra(global_args, device, autoencoder, trivial=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ac31b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[96mSetting up dataset\u001b[0m\n",
      "augs = Stereo(), PhaseFlipper()\n",
      "AudioDataset:1841824 files found.\n"
     ]
    }
   ],
   "source": [
    "hprint(\"Setting up dataset\")\n",
    "args = global_args\n",
    "args.training_dir = '/fsx/shawley/data/BDCT-0-chunk-48000'\n",
    "args.num_workers, args.batch_size = 6, 4\n",
    "\n",
    "train_set = AudioDataset([args.training_dir], load_frac=0.01)\n",
    "train_dl = torchdata.DataLoader(train_set, args.batch_size, shuffle=True,\n",
    "                num_workers=args.num_workers, persistent_workers=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1133e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam([*aa_model.reembedding.parameters()], lr=1e-10)  # Adam optimizer\n",
    "\n",
    "aa_model, opt, train_dl, autoencoder = accelerator.prepare(aa_model, opt, train_dl, autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f489156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1hqselat) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.076 MB of 0.076 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">hearty-mountain-16</strong>: <a href=\"https://wandb.ai/drscotthawley/a-a/runs/1hqselat\" target=\"_blank\">https://wandb.ai/drscotthawley/a-a/runs/1hqselat</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220911_054717-1hqselat/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1hqselat). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/fsx/shawley/code/audio-algebra/wandb/run-20220911_054901-10fbeshd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/drscotthawley/a-a/runs/10fbeshd\" target=\"_blank\">sunny-morning-17</a></strong> to <a href=\"https://wandb.ai/drscotthawley/a-a\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If logging to wandb, initialize the run\n",
    "#use_wandb = accelerator.is_main_process and args.name\n",
    "use_wandb = True\n",
    "if use_wandb:\n",
    "    import wandb\n",
    "    wandb.init(project='a-a', save_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f26fc4",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84b45b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4605 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, step: 0, loss: 615.449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/4605 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [256, 82, 5], expected input[4, 530, 65536] to have 82 channels, but got 530 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [84]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             log_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzmix_pca\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pca_point_cloud(zmix\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m demo_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 28\u001b[0m             log_dict \u001b[38;5;241m=\u001b[39m \u001b[43maa_demo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mautoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzsum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzmix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mlog(log_dict, step\u001b[38;5;241m=\u001b[39mstep)\n\u001b[1;32m     33\u001b[0m step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Input \u001b[0;32mIn [78]\u001b[0m, in \u001b[0;36maa_demo\u001b[0;34m(autoencoder, log_dict, zsum, zmix, step, demo_steps, sr)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog decoded audio for zsum and zmix\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m var,name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m([zsum, zmix],[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzsum\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzmix\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m----> 5\u001b[0m     fake_audio \u001b[38;5;241m=\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_it\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemo_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdemo_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m08\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m     fake_audio \u001b[38;5;241m=\u001b[39m fake_audio\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m32767\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mint16)\u001b[38;5;241m.\u001b[39mcpu()\n",
      "Input \u001b[0;32mIn [73]\u001b[0m, in \u001b[0;36mDiffusionDVAE.decode_it\u001b[0;34m(self, embeddings, demo_batch_size, demo_steps)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\u001b[38;5;241m==\u001b[39mdemo_batch_size: demo_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdemo_reals_shape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    154\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdemo_reals_shape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdemo_samples])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 155\u001b[0m fake_batches \u001b[38;5;241m=\u001b[39m \u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion_ema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemo_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m audio_out \u001b[38;5;241m=\u001b[39m rearrange(fake_batches, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb d n -> d (b n)\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# Put the demos together\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m audio_out\n",
      "File \u001b[0;32m~/envs/shazbot/lib64/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [73]\u001b[0m, in \u001b[0;36msample\u001b[0;34m(model, x, steps, eta, logits)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m trange(steps):\n\u001b[1;32m     33\u001b[0m \n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Get the model output (v, the predicted velocity)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 36\u001b[0m         v \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Predict the noise and the denoised image\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     pred \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m alphas[i] \u001b[38;5;241m-\u001b[39m v \u001b[38;5;241m*\u001b[39m sigmas[i]\n",
      "File \u001b[0;32m~/envs/shazbot/lib64/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/fsx/shawley/code/audio-diffusion/decoders/diffusion_decoder.py:136\u001b[0m, in \u001b[0;36mDiffusionAttnUnet1D.forward\u001b[0;34m(self, input, t, cond)\u001b[0m\n\u001b[1;32m    133\u001b[0m     cond \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(cond, (\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m], ), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    134\u001b[0m     inputs\u001b[38;5;241m.\u001b[39mappend(cond)\n\u001b[0;32m--> 136\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpqmf_bands \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    139\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpqmf\u001b[38;5;241m.\u001b[39minverse(outputs)\n",
      "File \u001b[0;32m~/envs/shazbot/lib64/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/shazbot/lib64/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/envs/shazbot/lib64/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/shazbot/lib64/python3.8/site-packages/blocks/blocks.py:15\u001b[0m, in \u001b[0;36mResidualBlock.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip(\u001b[38;5;28minput\u001b[39m)\n",
      "File \u001b[0;32m~/envs/shazbot/lib64/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/shazbot/lib64/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/envs/shazbot/lib64/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/envs/shazbot/lib64/python3.8/site-packages/torch/nn/modules/conv.py:307\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/shazbot/lib64/python3.8/site-packages/torch/nn/modules/conv.py:303\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    301\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    302\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 82, 5], expected input[4, 530, 65536] to have 82 channels, but got 530 channels instead"
     ]
    }
   ],
   "source": [
    "epoch, step = 0, 0  \n",
    "zspec_every, demo_every = 200, 200 \n",
    "while epoch < 1:  # training loop\n",
    "    for batch in tqdm(train_dl, disable=not accelerator.is_main_process):\n",
    "        opt.zero_grad()\n",
    "        #print(\"batch.shape = \",batch.shape)\n",
    "        stems, faders = get_stems_faders(batch, train_dl)\n",
    "        #print(\"stems[0].shape, faders =\",stems[0].shape, faders)\n",
    "\n",
    "        zsum, zmix, zarchive = accelerator.unwrap_model(aa_model).forward(stems,faders)  # .forward() for AA model\n",
    "        loss = accelerator.unwrap_model(aa_model).loss(zsum, zmix, zarchive)\n",
    "        accelerator.backward(loss)\n",
    "        opt.step()\n",
    "\n",
    "        if accelerator.is_main_process:\n",
    "            if step % 500 == 0: tqdm.write(f'Epoch: {epoch}, step: {step}, loss: {loss.item():g}')\n",
    "\n",
    "            if use_wandb:\n",
    "                log_dict = { 'epoch': epoch, 'loss': loss.item() }\n",
    "                \n",
    "                if step % zspec_every == 0:\n",
    "                    log_dict['zsum_spec'] = wandb.Image(tokens_spectrogram_image(zsum.detach()))\n",
    "                    log_dict['zmix_spec'] = wandb.Image(tokens_spectrogram_image(zmix.detach()))\n",
    "                    log_dict['zsum_pca'] = pca_point_cloud(zsum.detach().float())\n",
    "                    log_dict['zmix_pca'] = pca_point_cloud(zmix.detach().float())\n",
    "\n",
    "                if step % demo_every == 0:\n",
    "                    log_dict = aa_demo(autoencoder, log_dict, zsum, zmix, step)\n",
    "\n",
    "                    \n",
    "                wandb.log(log_dict, step=step)\n",
    "\n",
    "        step += 1\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a7ab88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='22.394 MB of 22.394 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>▅▇▁▁▂▅▁▅▅▇▇▁▇▅▅▁▁▁▄▂▃▃▇▁▅▅▆▁▅▃▂▅▂█▁▂▃▂▁▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>0</td></tr><tr><td>loss</td><td>567.12512</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">daily-aardvark-15</strong>: <a href=\"https://wandb.ai/drscotthawley/a-a/runs/2mhgkg83\" target=\"_blank\">https://wandb.ai/drscotthawley/a-a/runs/2mhgkg83</a><br/>Synced 6 W&B file(s), 118 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220911_053717-2mhgkg83/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if use_wandb: wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62783775",
   "metadata": {},
   "source": [
    "## Older: Main execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40429a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def main():\n",
    "\n",
    "    args = get_all_args()\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    try:\n",
    "        mp.set_start_method(args.start_method)\n",
    "    except RuntimeError:\n",
    "        pass\n",
    "\n",
    "    accelerator = accelerate.Accelerator()\n",
    "    device = accelerator.device\n",
    "    hprint = HostPrinter(accelerator)\n",
    "    hprint(f'Using device: {device}')\n",
    "\n",
    "    encoder_choices = ['ad','icebox']\n",
    "    encoder_choice = encoder_choices[0]\n",
    "    hprint(f\"Using {encoder_choice} as encoder\")\n",
    "    if 'icebox' == encoder_choice:\n",
    "        args.latent_dim = 64  # overwrite latent_dim with what Jukebox requires\n",
    "        encoder = IceBoxModel(args, device)\n",
    "    elif 'ad' == encoder_choice:\n",
    "        dvae = DiffusionDVAE(args, device)\n",
    "        #dvae = setup_weights(dvae, accelerator, device)\n",
    "        #encoder = dvae.encoder\n",
    "        #freeze(dvae)\n",
    "\n",
    "    hprint(\"Setting up AA model\")\n",
    "    aa_model = AudioAlgebra(args, device, dvae)\n",
    "\n",
    "    hprint(f'  AA Model Parameters: {n_params(aa_model)}')\n",
    "\n",
    "    # If logging to wandb, initialize the run\n",
    "    use_wandb = accelerator.is_main_process and args.name\n",
    "    if use_wandb:\n",
    "        import wandb\n",
    "        config = vars(args)\n",
    "        config['params'] = n_params(aa_model)\n",
    "        wandb.init(project=args.name, config=config, save_code=True)\n",
    "\n",
    "    opt = optim.Adam([*aa_model.reembedding.parameters()], lr=4e-5)\n",
    "\n",
    "    hprint(\"Setting up dataset\")\n",
    "    train_set = MultiStemDataset([args.training_dir], args)\n",
    "    train_dl = torchdata.DataLoader(train_set, args.batch_size, shuffle=True,\n",
    "                               num_workers=args.num_workers, persistent_workers=True, pin_memory=True)\n",
    "\n",
    "    hprint(\"Calling accelerator.prepare\")\n",
    "    aa_model, opt, train_dl, dvae = accelerator.prepare(aa_model, opt, train_dl, dvae)\n",
    "\n",
    "    hprint(\"Setting up frozen encoder model weights\")\n",
    "    dvae = setup_weights(dvae, accelerator)\n",
    "    freeze(accelerator.unwrap_model(dvae))\n",
    "    #encoder = dvae.encoder \n",
    "\n",
    "    hprint(\"Setting up wandb\")\n",
    "    if use_wandb:\n",
    "        wandb.watch(aa_model)\n",
    "\n",
    "    hprint(\"Checking for checkpoint\")\n",
    "    if args.ckpt_path:\n",
    "        ckpt = torch.load(args.ckpt_path, map_location='cpu')\n",
    "        accelerator.unwrap_model(aa_model).load_state_dict(ckpt['model'])\n",
    "        opt.load_state_dict(ckpt['opt'])\n",
    "        epoch = ckpt['epoch'] + 1\n",
    "        step = ckpt['step'] + 1\n",
    "        del ckpt\n",
    "    else:\n",
    "        epoch = 0\n",
    "        step = 0\n",
    "\n",
    "    # all set up, let's go\n",
    "    hprint(\"Let's go...\")\n",
    "    try:\n",
    "        while True:  # training loop\n",
    "            #print(f\"Starting epoch {epoch}\")\n",
    "            for batch in tqdm(train_dl, disable=not accelerator.is_main_process):\n",
    "                batch = batch[0]       # first elem is the audio, 2nd is the filename which we don't need\n",
    "                #hprint(f\"e{epoch} s{step}: got batch. batch.shape = {batch.shape}\")\n",
    "                opt.zero_grad()\n",
    "\n",
    "                # \"batch\" is actually not going to have all the data we want. We could rewrite the dataloader to fix this,\n",
    "                # but instead I just added get_stems_faders() which grabs \"even more\" audio to go with \"batch\"\n",
    "                stems, faders = get_stems_faders(batch, train_dl)\n",
    "\n",
    "                zsum, zmix, zarchive = accelerator.unwrap_model(aa_model).forward(stems,faders)  # Here's the model's .forward\n",
    "                loss = accelerator.unwrap_model(aa_model).loss(zsum, zmix, zarchive)\n",
    "                accelerator.backward(loss)\n",
    "                opt.step()\n",
    "\n",
    "                if accelerator.is_main_process:\n",
    "                    if step % 25 == 0:\n",
    "                        tqdm.write(f'Epoch: {epoch}, step: {step}, loss: {loss.item():g}')\n",
    "\n",
    "                    if use_wandb:\n",
    "                        log_dict = {\n",
    "                            'epoch': epoch,\n",
    "                            'loss': loss.item(),\n",
    "                            #'lr': sched.get_last_lr()[0],\n",
    "                            'zsum_pca': pca_point_cloud(zsum.detach()),\n",
    "                            'zmix_pca': pca_point_cloud(zmix.detach())\n",
    "                        }\n",
    "\n",
    "                        if (step % args.demo_every == 0):                                                    \n",
    "                            hprint(\"\\nMaking demo stuff\")\n",
    "\n",
    "                            mix_filename = f'mix_{step:08}.wav'\n",
    "                            reals = zarchive['mix'].clamp(-1, 1).mul(32767).to(torch.int16).cpu()\n",
    "                            reals = rearrange(reals, 'b d n -> d (b n)')\n",
    "                            print(\"reals.shape = \",reals.shape)\n",
    "                            torchaudio.save(mix_filename, reals, args.sample_rate)\n",
    "                            log_dict['mix'] = wandb.Audio(mix_filename, sample_rate=args.sample_rate, caption='mix')\n",
    "\n",
    "                            #demo(accelerator.unwrap_model(dvae), log_dict, zsum.detach(), zmix.detach(),  batch.shape[-1], step)\n",
    "                            zsum = zarchive['z0sum'].detach() # rearrange(zarchive['z0sum'], 'b n d -> b d n').detach()\n",
    "                            zmix = zarchive['z0mix'].detach() #rearrange(zarchive['z0mix'], 'b n d -> b d n').detach()\n",
    "\n",
    "                            hprint(f\"zsum.shape = {zsum.shape}\")\n",
    "                            noise = torch.randn([zsum.shape[0], 2, batch.shape[-1]]).to(accelerator.device)\n",
    "                            accelerator.unwrap_model(dvae).diffusion_ema.to(accelerator.device)\n",
    "                            model_fn = make_cond_model_fn(accelerator.unwrap_model(dvae).diffusion_ema, zsum)\n",
    "                            hprint(f\"noise.shape = {noise.shape}\")\n",
    "\n",
    "                            # Run the sampler\n",
    "                            with torch.cuda.amp.autocast():\n",
    "                                hprint(\"Calling sampler for zsum\")\n",
    "                                fakes = sample(accelerator.unwrap_model(dvae).diffusion_ema, noise, args.demo_steps, 1, zsum)\n",
    "                            fakes = rearrange(fakes, 'b d n -> d (b n)')\n",
    "                            zsum_filename = f'zsum_{step:08}.wav'\n",
    "                            fakes = fakes.clamp(-1, 1).mul(32767).to(torch.int16).cpu()\n",
    "                            torchaudio.save(zsum_filename, fakes, args.sample_rate)\n",
    "                            log_dict['zsum'] = wandb.Audio(zsum_filename, sample_rate=args.sample_rate, caption='zsum')\n",
    "                            \n",
    "                            with torch.cuda.amp.autocast():\n",
    "                                hprint(\"Calling sampler for zmix\")\n",
    "                                fakes = sample(accelerator.unwrap_model(dvae).diffusion_ema, noise, args.demo_steps, 1, zmix)\n",
    "                            fakes = rearrange(fakes, 'b d n -> d (b n)')\n",
    "                            zmix_filename = f'zmix_{step:08}.wav'\n",
    "                            fakes = fakes.clamp(-1, 1).mul(32767).to(torch.int16).cpu()\n",
    "                            torchaudio.save(zmix_filename, fakes, args.sample_rate)\n",
    "                            log_dict['zmix'] = wandb.Audio(zmix_filename, sample_rate=args.sample_rate, caption='zmix')\n",
    "                            hprint(\"Done making demo stuff\")\n",
    "                            \n",
    "                    if use_wandb: wandb.log(log_dict, step=step)\n",
    "\n",
    "                if step > 0 and step % args.checkpoint_every == 0:\n",
    "                    save(accelerator, args, aa_model, opt, epoch, step)\n",
    "\n",
    "                step += 1\n",
    "            epoch += 1\n",
    "    except RuntimeError as err:  # ??\n",
    "        import requests\n",
    "        import datetime\n",
    "        ts = datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        resp = requests.get('http://169.254.169.254/latest/meta-data/instance-id')\n",
    "        hprint(f'ERROR at {ts} on {resp.text} {device}: {type(err).__name__}: {err}', flush=True)\n",
    "        raise err\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520ff373",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Not needed if listed in console_scripts in settings.ini\n",
    "if __name__ == '__main__' and \"get_ipython\" not in dir():  # don't execute in notebook\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61fdcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
