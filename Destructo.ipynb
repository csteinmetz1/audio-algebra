{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "skip_showdoc: true\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Destructo. üí£üí•ü§Ø  v0.1\n",
    "...for *mathemangling* neural audio embeddings\n",
    "\n",
    "**Description:** This audio autoencoder demo can serve as a starting point for exploring operations on \"tokens\"/\"embeddings\" produced in the \"middle\" of the autoencoder.  The results may not sound \"good\", but may sound interesting! \n",
    "\n",
    ">NOTE: *This is intended to be a generic audio embeddings transformer/explorer, **not limited to one model**. \n",
    "Currently it only does DiffusionDVAE but Jukebox & RAVE support exist in another version yet to merged with this one.* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Authorship:** notebook by [Scott H. Hawley](https://twitter.com/drscotthawley)\n",
    "....using [audio-diffusion](https://github.com/zqevans/audio-diffusion)  by [Zach Evans](https://twitter.com/zqevans);  based on v-diffusion codes by [Katherine Crowson](https://twitter.com/rivershavewings), and incorporating [audio-diffusion-pytorch](https://github.com/archinetai/audio-diffusion-pytorch) by [Flavio Schneider](https://twitter.com/flavioschneide).\n",
    "...with notebook-styling ideas from [David Marx](https://mobile.twitter.com/digthatdata)'s [\"Video_Killed_The_Radio_Star_Defusion\"](https://colab.research.google.com/github/dmarx/video-killed-the-radio-star/blob/main/Video_Killed_The_Radio_Star_Defusion.ipynb#scrollTo=oPbeyWtesAoh) notebook. ;-) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "(The following links only work on \"normal\" notebook hosts, not Colab. On Colab, you can expand the TOC in the left sidebar by clicking the \"three rows of dots and lines\" icon.)\n",
    "1. üõ†Ô∏è [Setup](#setup)\n",
    "1. üîà [Supply Your Audio](#supply-audio)\n",
    "1. üóú [Encode Audio into Neural Embeddings](#encode)\n",
    "1. üå™ [Mathemangle the Embeddings](#mathemangle)\n",
    "1. üîä [Decode and Listen](#decode)\n",
    "1. ‚ÜóÔ∏è [Share Your New Sounds (TODO)](#share)\n",
    "1. ‚öñÔ∏è [Licence](#license)\n",
    "1. ‚ùî [Audio-Algebra for Effects?](#audalgeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"setup\"></a>\n",
    "# $1.$ üõ†Ô∏è Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Installation\n",
    "install = True  # can set to false to skip this part, e.g. for re-running in same session\n",
    "if install:     # ffmpeg is to add MP3 support to Colab\n",
    "    !yes | sudo apt install ffmpeg \n",
    "    !pip install -Uqq einops gdown \n",
    "    !pip install -Uqq git+https://github.com/drscotthawley/aeiou\n",
    "    !pip install -Uqq git+https://github.com/drscotthawley/audio-algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "import math\n",
    "import os  \n",
    "import subprocess\n",
    "from collections import namedtuple\n",
    "import numpy as np \n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch import optim, nn, Tensor\n",
    "from torch import multiprocessing as mp\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data as torchdata\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from aeiou.core import load_audio, makedir, batch_it_crazy\n",
    "from aeiou.viz import *    # trust me\n",
    "from audio_algebra.DiffusionDVAE import DiffusionDVAE, sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Check: GPU Info\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Compute device is {device}\\n\")\n",
    "if 'cuda' == str(device): \n",
    "    !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Choose Your Model (only 1 choice now)\n",
    "model_choices = {  # dict of possible models\n",
    "    'DiffusionDVAE':{'ckpt_url':'https://drive.google.com/file/d/1C3NMdQlmOcArGt1KL7pH32KtXVCOfXKr/view?usp=sharing',\n",
    "                     'ckpt_hash':'6a304c3e89ea3f7ca023f4c9accc5df8de0504595db41961cc7e8b0d07876ef5',\n",
    "                     'gdrive_path':'MyDrive/AI/checkpoints/DiffusionDVAE.ckpt',\n",
    "                     }\n",
    "}\n",
    "\n",
    "model_choice = 'DiffusionDVAE' #@param [\"DiffusionDVAE\"]\n",
    "mc = model_choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model Checkpoint\n",
    "\n",
    "The checkpoint file is 4 GB. \n",
    "\n",
    "The notebook will download the checkpoint file if you're not on Colab. (This is slow).\n",
    "\n",
    "On Colab, you can manually mount the checkpoint directly from Google Drive. (This is fast). To do that, click [this Google Drive link](https://drive.google.com/file/d/1C3NMdQlmOcArGt1KL7pH32KtXVCOfXKr/view?usp=sharing), then choose \"Add to My Drive\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Mount or Download Checkpoint\n",
    "on_colab = os.path.exists('/content')\n",
    "if on_colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive/') \n",
    "    ckpt_file = '/content/drive/'+model_choices[mc]['gdrive_path']\n",
    "    while not os.path.exists(ckpt_file):\n",
    "        print(f\"\\nPROBLEM: Expected to find the checkpoint file at {ckpt_file} but it's not there.\\nWhere is it? (Go to the File system in the left sidebar and find it)\")\n",
    "        ckpt_file = input('Enter location of checkpoint file: ')\n",
    "else:\n",
    "    ckpt_file = 'checkpoint.ckpt'\n",
    "    if not os.path.exists(ckpt_file):\n",
    "        url = model_choices[mc]['ckpt_url']\n",
    "        # downloading large files from GDrive requires special treatment to bypass the dialog button it wants to throw up\n",
    "        id = url.split('/')[-2]\n",
    "        #cmd = f'wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate \\'https://docs.google.com/uc?export=download&id={id}\\' -O- | sed -rn \\'s/.*confirm=([0-9A-Za-z_]+).*/\\1\\\\n/p\\')&id={id}\" -O {ckpt_file} && rm -rf /tmp/cookies.txt'\n",
    "        #subprocess.run(cmd, shell=True, check=True) \n",
    "        !gdown -O {ckpt_file} {id}\n",
    "\n",
    "        print(f\"\\nSecurity: checking hash on checkpoint file...\")\n",
    "        new_hash = subprocess.run(['shasum', '-a','256',ckpt_file], stdout=subprocess.PIPE).stdout.decode('utf-8').split(' ')[0]\n",
    "        #new_hash = subprocess.run(['md5sum',ckpt_file], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "        assert new_hash == model_choices[mc]['ckpt_hash'], \"Hashes don't match. STOP THE NOTEBOOK. DO NOT EXECUTE.\"\n",
    "        print(\"[Hash is fast,] Hash is cool.  Ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part actually loads the checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title \n",
    "print(f\"Now loading checkpoint from {model_choices[mc]['gdrive_path']}\")\n",
    "\n",
    "args_dict = {'num_quantizers':0, 'sample_size': 65536, 'sample_rate':48000, 'latent_dim': 64, 'pqmf_bands':1, 'ema_decay':0.995, 'num_quantizers':0}\n",
    "global_args = namedtuple(\"global_args\", args_dict.keys())(*args_dict.values())\n",
    "\n",
    "model = DiffusionDVAE.load_from_checkpoint(ckpt_file, global_args=global_args)\n",
    "model.eval() # Inference mode\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"supply-audio\"></a>\n",
    "# $2.$ üîà Supply Your Audio\n",
    "We need the filename (full path) to some audio already on your hard drive if you're running locally, or else something that you upload if you're running this on a cloud service. \n",
    "* On Colab: To upload, you can click on the the file icon to the left, and find the upload icon, which is looks like a piece of paper with an upward arrow on it. \n",
    "* On Kaggle: Go to File > Upload Data. It'll make you create a \"dataset\"; afterward your file will be `/kaggle/input/<your_dataset_name>/<your_file>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Provide the full path of your *short* (3 to 10 sec) audio file: \n",
    "\n",
    "audio_file = '/content/shadow_sfx_1.mp3'  #@param{type:\"string\"}\n",
    "\n",
    "audio_in = load_audio(audio_file, sr=global_args.sample_rate)\n",
    "if audio_in.shape[0] == 1:     # our models expect stereo\n",
    "    audio_in = torch.vstack((audio_in, audio_in))\n",
    "\n",
    "print(f\"audio_in.shape =\",audio_in.shape)\n",
    "playable_spectrogram(audio_in, output_type='live')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"encode\"></a>\n",
    "# $3.$ üóú Encode Audio into Neural Embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Make batches and encode\n",
    "# First, we need to chop up the audio into batches \n",
    "demo_reals = batch_it_crazy(audio_in, global_args.sample_size)\n",
    "\n",
    "max_batch_size = 8\n",
    "if demo_reals.size()[0] > max_batch_size:\n",
    "    print(f\"Warning: Due to CUDA memory limits, we're cutting you off at a batch size of {max_batch_size}\")\n",
    "    demo_reals = demo_reals[0:max_batch_size,:,:]\n",
    "\n",
    "print(f\"demo_reals.shape = {demo_reals.shape}\\ni.e. {demo_reals.shape[0]} batches\")\n",
    "\n",
    "\n",
    "# TODO: move most of this stuff to an import \n",
    "def encode_it(demo_reals, module):\n",
    "    encoder_input = demo_reals\n",
    "    \n",
    "    if module.pqmf_bands > 1:\n",
    "        encoder_input = module.pqmf(demo_reals)\n",
    "    \n",
    "    encoder_input = encoder_input.to(module.device)\n",
    "    demo_reals = demo_reals.to(module.device)\n",
    "    noise = torch.randn([demo_reals.shape[0], 2, module.demo_samples]).to(module.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = module.encoder_ema(encoder_input)\n",
    "        if module.quantized:\n",
    "            #Rearrange for Memcodes\n",
    "            embeddings = rearrange(embeddings, 'b d n -> b n d')\n",
    "            embeddings, _= module.quantizer_ema(embeddings)\n",
    "            embeddings = rearrange(embeddings, 'b n d -> b d n')\n",
    "        \n",
    "        embeddings = torch.tanh(embeddings)\n",
    "        return embeddings, noise\n",
    "\n",
    "# attach some arg values to the model \n",
    "model.demo_samples = global_args.sample_size \n",
    "model.quantized = global_args.num_quantizers > 0\n",
    "\n",
    "embeddings, noise = encode_it(demo_reals, model) # ENCODING! \n",
    "\n",
    "print(f\"Encoded to embeddings.shape =\",embeddings.shape)\n",
    "print(f\"            and noise.shape =\",noise.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ## üîä Check: Decode Unaltered Embeddings\n",
    "#@markdown Perform reconstruction using  \\_\\_this many\\_\\_ diffusion inference steps:\n",
    "\n",
    "demo_steps = 35 #@param {type:\"slider\", min:10, max:100, step:1}\n",
    "\n",
    "fake_batches = sample(model.diffusion_ema, noise, demo_steps, 0, embeddings)\n",
    "\n",
    "audio_out = rearrange(fake_batches, 'b d n -> d (b n)') # Put the demos together\n",
    "playable_spectrogram(audio_out, output_type='live')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "....Not a perfect reconstruction, right?  But pretty close. We're working on increasing the reconstruction quality, but for now that's the best you'll get.  That's not the interest part though!  Keep going... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mathemangle\"></a>\n",
    "# $4.$üå™ Mathemangle the Embeddings\n",
    "..and reap the whirlwind.\n",
    "\n",
    "First let's print out a little bit of info so you can see what your're working with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title  \n",
    "z = embeddings   # shorthand variable name \"z\"\n",
    "print(f\"Embeddings array has shape z.shape = {z.shape}: [b, d, n] = {z.shape[0]} batches, {z.shape[1]} dimensions, {z.shape[-1]} time-samples.\\nEach time-sample exists in a {z.shape[1]}-dimensional latent space\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Perform math operations in embedding space: \"Embedding-Based Audio Effects\"\n",
    "\n",
    "#@markdown Random number to use with some calculations:\n",
    "rand_fac = 0.5   #@param {type:\"number\"}\n",
    "\n",
    "z = embeddings.clone() # make a copy before doing math\n",
    "\n",
    "\n",
    "# 'MATH OPTIONS' below for messing with the sound....\n",
    "\n",
    "\n",
    "#@markdown ###Effects Presets:\n",
    "\n",
    "#@markdown (These 'chain' if mutliple are enabled. To reorder them you can highlight and drag-and-drop in the code itself)\n",
    "\n",
    "call_and_response = True #@param {type:\"boolean\"}\n",
    "if call_and_response: z = -z + rand_fac*z*(2*torch.rand_like(z)-1)\n",
    "\n",
    "hurt_drums = False #@param {type:\"boolean\"}\n",
    "if hurt_drums: z = (1-rand_fac)*embeddings + rand_fac*z*(2*torch.rand_like(z)-1) \n",
    "\n",
    "swap_emb_dims = True #@param {type:\"boolean\"}\n",
    "if swap_emb_dims: z = z.flip(dims=[1])     # Swap across the dimensions\n",
    "\n",
    "Destructo = False #@param {type:\"boolean\"}\n",
    "if Destructo: z = torch.max(z)*(torch.sign(z) - z) # Scott calls this one 'Destructo'\n",
    "\n",
    "Destructo2 = False #@param {type:\"boolean\"}\n",
    "if Destructo2: z = torch.max(torch.abs(z)) - z\n",
    "\n",
    "big_changes = False #@param {type:\"boolean\"}\n",
    "if big_changes: z = 2*z                         # 'big changes afoot'\n",
    "\n",
    "wavy = False #@param {type:\"boolean\"}\n",
    "if wavy: z = z*torch.cos(torch.linspace(0,4*6.28,z.shape[-1])).to(device)             # slow sine wave of embeddings? IDK\n",
    "\n",
    "time_reverse = False #@param {type:\"boolean\"}\n",
    "if time_reverse: z = torch.flip(z,[2])          # time reversal of tokens\n",
    "\n",
    "flippy = False #@param {type:\"boolean\"}\n",
    "if flippy: z = z.clone() + torch.flip(z,[-1])  # IDK what to calll it\n",
    "\n",
    "kill_half = False #@param {type:\"boolean\"}\n",
    "if kill_half: z[:,33:-1,:] = 0.\n",
    "\n",
    "reverb_time = 0 #@param {type:\"number\"}\n",
    "if reverb_time != 0:\n",
    "    for i in range(z.shape[-1]):   # exp. weighted moving average\n",
    "        z = z + math.exp(-i/reverb_time)*torch.nn.functional.pad(z,(i+1,0,0,0,0,0), mode='constant')[:,:,0:z.shape[-1]]\n",
    "\n",
    "overdrive_factor = 1 #@param {type:\"number\"}  # 1=do nothing\n",
    "if overdrive_factor!=1: z = torch.max(z)*torch.tanh(z*overdrive_factor)   # overdrive? \n",
    "# \n",
    "\n",
    "#@markdown WRITE YOUR OWN MATH OP! (use \"z\" as embedding variable)\n",
    "op = \"z = 1.0*z\" #@param {type:\"string\"}\n",
    "if op != '': exec(op)\n",
    "\n",
    "embeddings2 = z  # for backwards-compatibility with old version of notebook \n",
    "\n",
    "print(\"Just a quick pre-post masurement to see how much you futzed things up:\")\n",
    "print(f\"Before mathemangling, embeddings min & max values were {torch.min(embeddings).cpu().numpy()}, {torch.max(embeddings).cpu().numpy()}, respectively.\")\n",
    "print(f\"After mathemangling,  embeddings min & max values were {torch.min(z).cpu().numpy()}, {torch.max(z).cpu().numpy()}, respectively.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåå Check: Visualize pre/post embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tokens_spectrogram_image(embeddings))\n",
    "display(tokens_spectrogram_image(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3D Point Clouds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before:\")\n",
    "show_pca_point_cloud(embeddings, mode='lines+markers') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After:\")\n",
    "show_pca_point_cloud(z, mode='lines+markers') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"decode\"></a>\n",
    "# $5.$ üîä Decode and Listen\n",
    "Now we decode & listen to the embeddings that were the result of the mathemangling operation(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_steps = 35 #@param {type:\"slider\", min:10, max:100, step:1}\n",
    "\n",
    "fake_batches2 = sample(model.diffusion_ema, noise, demo_steps, 0, z)\n",
    "audio_out2 = rearrange(fake_batches2, 'b d n -> d (b n)').cpu() # un-batch: Put the demos together\n",
    "if np.abs(audio_out2.flatten()).max() >= 1.0:                   # let's go ahead and rescale to prevent clipping \n",
    "    audio_out2 = 0.99*audio_out2/np.abs(audio_out2.flatten()).max() \n",
    "playable_spectrogram(audio_out2, output_type='live')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Input Audio was (for comparison):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playable_spectrogram(audio_in, output_type='live')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title (Optional) Re-amp your output?\n",
    "#@markdown With re-amping, you create a feedback loop so you can go back up to \"Mathemangle\" to add more to your signal chain.\n",
    "\n",
    "# @markdown (Note that this will overwrite your input embeddings, so we'll save a backup just in case you want to \"undo\")\n",
    "\n",
    "re_amp = False  #@param {type:\"boolean\"}\n",
    "if re_amp:\n",
    "    backup, embeddings = embeddings.clone(), z "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"share\"></a>\n",
    "# $6.$ ‚ÜóÔ∏è Share Your New Sounds \n",
    "...either alone as part of a song you made! [Harmonai](https://www.harmonai.org/) Play Discord has a [#show-and-tell](https://discord.com/channels/1001555636569509948/1013162448724951073) channel where you can post your Destructo creations! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"license\"></a>\n",
    "# $7.$ ‚öñÔ∏è Licence\n",
    "Sounds are all yours, period.  \n",
    "\n",
    "As for the code, it is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Licensed under the MIT License\n",
    "\n",
    "Copyright (c) 2022 Scott H. Hawley \n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "<a id=\"audalgeff\"></a>\n",
    "# $8.$ ‚ùî Afterword: Researching Audio-Algebra for Effects?\n",
    "\n",
    "## Can We 1-Shot-Learn Audio Effects using These Embeddings? (Spoiler: No but let's try...)\n",
    "\n",
    "Do embeddings store anything important, and can vectors in embedding-space be meaningfully used to model audio effects?\n",
    "\n",
    "Language models showed that operations on embeddings of words (for well-trained masked language models) could produce \"semantically meaningful\" results, such as \"king - man + woman = queen\" or \"japan - tokyo = germany - berlin\". \n",
    "\n",
    "Can our model(s) do similar things for audio?  Here we'll subtract two (new) embeddings and add them to a third (your original input embedding).  What do we get? \n",
    "\n",
    "In what follows, you'll upload two \"versions\" of the same audio, with (\"wet\") and without (\"dry\") an effect applied (e.g. distortion).\n",
    "\n",
    "> Will we get e.g. \"wet_piano - dry_piano + dry_guitar = wet_guitar\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown Provide the full path of your audio files:\n",
    "\n",
    "dry_name = '/content/speech_test.wav'  #@param{type:\"string\"}\n",
    "dry = load_audio(dry_name, sr=global_args.sample_rate)\n",
    "if dry.shape[0] == 1: dry = torch.vstack((dry, dry))\n",
    "\n",
    "wet_name = '/content/speech_wahwah.wav' #@param{type:\"string\"}\n",
    "wet = load_audio(wet_name, sr=global_args.sample_rate)\n",
    "if wet.shape[0] == 1: wet = torch.vstack((wet, wet))\n",
    "\n",
    "# let's make them the same size \n",
    "min_len = min(dry.shape[-1], wet.shape[-1])\n",
    "dry, wet = dry[:,:min_len], wet[:,:min_len]\n",
    "\n",
    "print(f\"dry.shape = {dry.shape}, wet.shape = {wet.shape}\")\n",
    "print(\"dry:\")\n",
    "playable_spectrogram(dry, output_type='live')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"wet:\")  \n",
    "playable_spectrogram(wet, output_type='live')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In raw-audio space, the difference between these is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"wet - dry (difference taken in raw audio domain):\")  \n",
    "playable_spectrogram(wet - dry, output_type='live')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "....That includes a lot of the original \"content\".  One might hope that in embedding space, by \"subtracting out the content\" we might leave only the \"audio effect\" that makes up their difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Encode wet & dry, compute the difference\n",
    "\n",
    "dry_reals = batch_it_crazy(dry, global_args.sample_size)\n",
    "wet_reals = batch_it_crazy(wet, global_args.sample_size)\n",
    "\n",
    "max_batch_size = 8\n",
    "if dry_reals.size()[0] > max_batch_size:\n",
    "    print(f\"Warning: Due to CUDA memory limits, we're cutting you off at a batch size of {max_batch_size}\")\n",
    "    dry_reals = dry_reals[0:max_batch_size,:,:]\n",
    "    wet_reals = wet_reals[0:max_batch_size,:,:]\n",
    "\n",
    "print(f\"dry_reals.shape = {demo_reals.shape}, wet_reals.shape = {wet_reals.shape}\")\n",
    "\n",
    "dry_embeddings, dry_noise = encode_it(dry_reals, model) \n",
    "wet_embeddings, wet_noise = encode_it(wet_reals, model) \n",
    "diff = wet_embeddings - dry_embeddings     ## this is \"king - man\"\n",
    "\n",
    "print(f\"Difference encoded to diff.  diff.shape =\",diff.shape,\", and noise.shape =\",noise.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title üîä Appy difference to input audio, decode\n",
    "\n",
    "# @markdown Time-average the difference in embeddings?\n",
    "time_avg = False  #@param {type:\"boolean\"}\n",
    "\n",
    "# @markdown Number of steps for diffusion reconstruction: \n",
    "demo_steps = 35 #@param {type:\"slider\", min:10, max:100, step:1}\n",
    "\n",
    "z = embeddings.clone() # backup.clone()    # just in case we re-amped, go back to an earlier version\n",
    "\n",
    "if time_avg: \n",
    "    diff = diff.mean(axis=-1)\n",
    "else:\n",
    "    # make sure embeddings and diff have the same length\n",
    "    length_difference = z.shape[-1] - diff.shape[-1]\n",
    "    if length_difference > 0:  # zero-pad the end of diff\n",
    "        diff = torch.nn.functional.pad(diff,(length_difference,0,0,0,0,0), mode='constant')\n",
    "    elif length_difference < 0: # truncate diff \n",
    "        diff = diff[:,:,:z.shape[-1]]\n",
    "    \n",
    "    diff = diff.mean(0) # average diff over batches, to allow broadcasting\n",
    "\n",
    "\n",
    "# now apply the \"learned effect\"\n",
    "z = z + diff   # woman + (king - man)  = ? (queen?)\n",
    "\n",
    "# and decode\n",
    "fake_batches2 = sample(model.diffusion_ema, noise, demo_steps, 0, z)\n",
    "audio_out3 = rearrange(fake_batches2, 'b d n -> d (b n)').cpu() # un-batch: Put the demos together\n",
    "if np.abs(audio_out3.flatten()).max() >= 1.0:                   # let's go ahead and rescale to prevent clipping \n",
    "    audio_out3 = 0.99*audio_out3/np.abs(audio_out3.flatten()).max() \n",
    "playable_spectrogram(audio_out3, output_type='live')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...massive failure. Too much of the wet-dry audio content made it into the output, rather than just having the effect be applied.  \n",
    "\n",
    "So, *for this particular embedding model*, we did not observe a \"vector space\" relation for a particular (time-dependent) audio effect.  Perhaps a different model would allow us to perform such an operation. \n",
    "\n",
    "\n",
    "Even with this model, we might still try to compute an \"average\" displacement (or perhaps somehow a nonlinear effect) in embedding space for certain effects, and see how well that works.  This is an active goal of the Audio Algebra research program.  We can try that with a dataset in another notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
